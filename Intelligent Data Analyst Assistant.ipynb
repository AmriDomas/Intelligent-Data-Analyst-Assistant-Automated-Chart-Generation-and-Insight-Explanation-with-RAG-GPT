{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "254bfcd3",
   "metadata": {},
   "source": [
    "# Intelligent Data Analyst Assistant: Automated Chart Generation and Insight Explanation with RAG & GPT\n",
    "\n",
    "Muh Amri Sidiq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50fa6bc",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this project, I built an intelligent data analysis assistant that combines human-like reasoning with data-driven accuracy. By integrating Retrieval-Augmented Generation (RAG), GPT-4o, and Python data visualization libraries (like Matplotlib and Seaborn), the assistant is capable of understanding user questions, analyzing uploaded datasets (CSV or PDF), generating appropriate visualizations, and delivering meaningful insights all in real time.\n",
    "\n",
    "Users can upload structured data such as sales reports or other tabular formats, and the system will automatically:\n",
    " - Detect relevant columns and schema,\n",
    " - Generate one or more visualizations (bar, pie, line, timeseries, scatter, histogram, correlation matrix),\n",
    " - Apply filters and aggregation as needed,\n",
    " - Interpret chart results using custom auto-analysis logic (generate_auto_answer()),\n",
    " - And finally present a concise, human-readable summary.\n",
    "\n",
    "If the source data is from PDFs, the assistant retrieves and understands natural language passages and generates textual answers using GPT-based language reasoning.\n",
    "\n",
    "This assistant mimics how a business analyst would explore data, extract patterns, and communicate findings making it suitable for both business users and data professionals. The result is a powerful tool that enhances productivity, reduces manual work, and showcases the power of combining LLMs with structured data pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c77b90c",
   "metadata": {},
   "source": [
    "## Workflow RAG\n",
    "\n",
    "there are several steps to determine the RAG flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9572ac",
   "metadata": {},
   "source": [
    "### 1. Read Data\n",
    "\n",
    "the data we use is tabular data that contains ecomerce transaction columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e1caf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the e-commerce dataset\n",
    "# The dataset is assumed to be in the same directory as this script.\n",
    "ecommerce = pd.read_csv(\"data/US  E-commerce records 2020.csv\", encoding='cp1252')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32813d61",
   "metadata": {},
   "source": [
    "### 2. Data to Text Transformation\n",
    "\n",
    "So that the data can be used, which was originally tabular, it must first be converted into text form, so that LLM can read it. By combining all the columns of each row, the first step is to make the data of each row into 1 full sentence as a condition for LLM to read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a5a835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat sebuah fungsi\n",
    "def merge_column(df, column_data):\n",
    "    df['teks'] = df[column_data].astype('str').agg(' | '.join, axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d53ce54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    01-01-20 | 849 | CA-2017-107503 | Standard Cla...\n",
       "1    01-01-20 | 4010 | CA-2017-144463 | Standard Cl...\n",
       "2    01-01-20 | 6683 | CA-2017-154466 | First Class...\n",
       "3    01-01-20 | 8070 | CA-2017-151750 | Standard Cl...\n",
       "4    01-01-20 | 8071 | CA-2017-151750 | Standard Cl...\n",
       "Name: teks, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['Order Date', 'Row ID', 'Order ID', 'Ship Mode', 'Customer ID',\n",
    "        'Segment', 'Country', 'City', 'State', 'Postal Code',\n",
    "        'Region', 'Product ID', 'Category', 'Sub-Category', 'Product Name',\n",
    "        'Sales', 'Quantity', 'Discount', 'Profit']\n",
    "\n",
    "merge_column(df=ecommerce, column_data=cols)\n",
    "ecommerce['teks'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5783129",
   "metadata": {},
   "source": [
    "### 3. Search for Text Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6978c459",
   "metadata": {},
   "source": [
    "#### 3.1 Embedding Teks\n",
    "\n",
    "At this stage we convert the text into a vector representation capturing the meaning of the text. In this case looking for similarities between one data and another.\n",
    "\n",
    "Information about the similarity between one data and another is important because the initial goal of RAG is to retrieve data information that is relevant to the given question, in order to generate relevant LLM answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31954776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the embedding model to be used\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2ce76d",
   "metadata": {},
   "source": [
    "`query =  \"what segment makes the most purchases in the state of California?\"`\n",
    "\n",
    "`embedding_query_new = model.encode(query, convert_to_numpy=True)`\n",
    "\n",
    "Setting up a query and embedding it for the information retrieval process.\n",
    "The purpose of this code is to convert the text question (\"query\"). into a vector representation (embedding) so that it can be compared with data already indexed by FAISS. With this representation, the system can search for the most relevant rows of data based on similarity of meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2e099ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare query & embedding new\n",
    "query =  \"what segment makes the most purchases in the state of California?\"\n",
    "embedding_query_new = model.encode(query, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff63012",
   "metadata": {},
   "source": [
    "`embedding_dataframe = model.encode(ecommerce['teks'], convert_to_numpy=True)`\n",
    "\n",
    "This code serves to convert the text in the text column of the ecommerce DataFrame into a vector representation (embedding) using the SentenceTransformer model. This embedding is important for performing semantic search with FAISS\n",
    " - model.encode(...): Calls the model to convert the text to embedding.\n",
    " - ecommerce[‚Äòtext‚Äô]: A column containing combined information from multiple\n",
    " - columns, prepared beforehand with a function like transform_data.\n",
    "\n",
    "convert_to_numpy=True: Sets the output to be in numpy array format, which is required for use in FAISS index or other mathematical operations. The main purpose of this step is to prepare the data so that it can be searched based on similarity of meaning, not just keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "084a9c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding for our data\n",
    "embedding_dataframe = model.encode(ecommerce['teks'], convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a40e8aa",
   "metadata": {},
   "source": [
    "`cosine_scores = util.cos_sim(embedding_query_new, embedding_dataframe)`\n",
    "\n",
    "This code is used to calculate the semantic similarity between the query and all data in the embedding form.\n",
    "\n",
    "Details:\n",
    " - embedding_query_new: The representation vector of the query that has been converted to embedding.\n",
    " - embedding_dataframe: Set of embeddings of all text data in the text column of the DataFrame.\n",
    " - util.cos_sim(...): A function from sentence_transformers.util that calculates cosine similarity, a measure of how similar two direction vectors are in multidimensional space.\n",
    "\n",
    "Main objective: Find out how relevant each line of data is to the user's question based on closeness of meaning, not just word similarity.\n",
    "\n",
    "Output:\n",
    " - cosine_scores produces a matrix with values between -1 and 1.\n",
    " - A value close to 1 means very similar, close to 0 means not similar, and a negative value means very different in semantic direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5190cd02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1494, 0.2476, 0.0978,  ..., 0.3054, 0.3561, 0.2677]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate cosine similarity\n",
    "cosine_scores = util.cos_sim(embedding_query_new, embedding_dataframe)\n",
    "cosine_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5836b620",
   "metadata": {},
   "source": [
    "#### 3.2: FAISS Indexing From Question\n",
    "\n",
    "`import faiss, import numpy as np` This line imports two important libraries:\n",
    " - `faiss`: Used for indexing and similarity-based vector search.\n",
    " - `numpy`: Used for numeric array manipulation, such as vector normalization.\n",
    "\n",
    "`embedding_dataframe = embedding_dataframe / np.linalg.norm(embedding_dataframe, axis=1, keepdims=True)`\n",
    "`embedding_dataframe = embedding_dataframe.astype('float32')`\n",
    "Manual normalization of embedding\n",
    " - Normalization is done so that each embedding vector has a length (norm) = 1.\n",
    " - This is important because FAISS uses inner product (dot product) to calculate similarity, and when vectors are normalized, dot product = cosine similarity\n",
    " - astype(‚Äòfloat32‚Äô) is required because FAISS only accepts float32 data type.\n",
    "   \n",
    "   üîç This normalization ensures the similarity search in FAISS is equivalent to cosine similarity.\n",
    "\n",
    "`dimension = embedding_dataframe.shape[1]` Specifying Embedding Dimensions\n",
    " - Retrieves the number of dimensions of the embedding (for example, 384 if using the 'paraphrase-MiniLM-L6-v2' model).\n",
    " - This value is needed when creating the FAISS index:\n",
    "    This dimension is the size of the vector space where all embeddings are placed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1df12cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the cosine similarity scores with manually\n",
    "embedding_dataframe = embedding_dataframe / np.linalg.norm(embedding_dataframe, axis=1, keepdims=True)\n",
    "embedding_dataframe = embedding_dataframe.astype('float32')\n",
    "\n",
    "# take value embedding\n",
    "dimension = embedding_dataframe.shape[1]\n",
    "dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff7a156",
   "metadata": {},
   "source": [
    "`index = faiss.IndexFlatL2(dimension)` Creating and Populating the FAISS Index\n",
    "\n",
    "This line makes the FAISS index use L2 (Euclidean) distance as the similarity metric.\n",
    "Explanation:\n",
    " - IndexFlatL2 is a FAISS index type that calculates the Euclidean distance (L2 distance) between vectors.\n",
    " - dimension is the number of dimensions of each embedding vector (e.g. 384).\n",
    " - This index does not use fast search structures (such as IVF or HNSW) - suitable for small to medium datasets.\n",
    "\n",
    "   üìå Use IndexFlatIP if you want to calculate cosine similarity with normalized vectors.\n",
    "\n",
    "`index.add(embedding_dataframe)` This line adds all embedding vectors to the FAISS index.\n",
    "Explanation:\n",
    " - embedding_dataframe contains all the encoded vectors of the text in the dataset.\n",
    " - Once added, the index is ready to be used for similarity-based queries.\n",
    "\n",
    "   FAISS can now be used to find the most similar data to the given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2a19e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embedding_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9db6e6c",
   "metadata": {},
   "source": [
    "The dimension results if compared back to the example query question above, will be as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c51cca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what segment makes the most purchases in the state of California?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3181ac77",
   "metadata": {},
   "source": [
    "`embedding_index_query = model.encode([query])`\n",
    "\n",
    "Explanation:\n",
    " - This code is used to convert a user's query into an embedding vector, so that it can be compared with vectors of previously indexed data.\n",
    " - `model.encode(...)`: A function of the SentenceTransformer that converts text into a numerical representation (embedding).\n",
    " - `[query]`: Given in list form because the model expects the input to be a list of strings, even if it is only one query.\n",
    " - The result is a 2-dimensional array (shape: [1, dimension]), which is the format FAISS needs for the search process.\n",
    " \n",
    " Main objective: Convert the user's question into a vector format, so that it can be compared against the entire data to find the most relevant answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c942c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index_query = model.encode([query])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3bb20a",
   "metadata": {},
   "source": [
    "`D, I = index.search(embedding_index_query, k = 2)`\n",
    "\n",
    "Explanation:\n",
    " - This code is used to find the k closest (most similar) vectors in the FAISS index to the query that has been converted to embedding.\n",
    " - `embedding_index_query`: The encoding result vector of the user query.\n",
    " - `k = 2`: Retrieve the 2 closest results (top-2 most similar).\n",
    " - `index.search(...)`: FAISS function to perform a distance-based search (e.g. Euclidean if using IndexFlatL2, or cosine if using IndexFlatIP + normalization).\n",
    "\n",
    "Output:\n",
    " - `D`: The distance matrix or similarity score between the query and the k closest results.\n",
    " - `I`: The row index matrix of the original data that is most similar to the query.\n",
    "\n",
    "Main objective: Find the rows of data in the dataset that are most relevant to the user's query based on embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b69de56",
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = index.search(embedding_index_query, k = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11c6bdb",
   "metadata": {},
   "source": [
    "`D:` the distance between the `query_vector` and the nearest vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91e251fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44.061226, 44.409435]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b4d2b8",
   "metadata": {},
   "source": [
    "`I:` index of most similar vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6424b505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3178,  411]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dfb39d",
   "metadata": {},
   "source": [
    "Function: Build FAISS Index with Cosine Similarity\n",
    "\n",
    "Function Explanation:\n",
    "The build_faiss_index_cosine(text) function is used to create a FAISS index based on cosine similarity, which is useful in the process of searching relevant data based on the similarity of text meaning.\n",
    "\n",
    "Steps:\n",
    "1. Embedding Text: `embedding = model.encode(teks , convert_to_numpy=True)`\n",
    "   Convert a text list (text) into a numeric vector (embedding) using SentenceTransformer.\n",
    "2. Normalization for Cosine Similarity: `embedding = embedding / np.linalg.norm(embedding, axis=1, keepdims=True)`,\n",
    "`embedding = embedding.astype('float32')`. \n",
    "   - Normalize each vector to unit-norm so that dot product = cosine similarity.  - Convert to float32 as FAISS only supports this data type.\n",
    "3. Create and Fill Index:\n",
    "   `index = faiss.IndexFlatL2(dimension)`\n",
    "    `index.add(embedding)`\n",
    "   - Creating FAISS index is based on L2 distance, but since the vectors are already normalized, the L2 search is equivalent to cosine similarity.\n",
    "   - dimension is the number of dimensions of the embedding vector (e.g. 384 or 768 depending on the model).\n",
    "4. Return:\n",
    "   - Returns the FAISS index and its embedding for use in the search process.\n",
    "\n",
    "Notes:\n",
    "- FAISS does not provide IndexFlatCosine function, so to do cosine similarity, we need to do manual normalization and still use IndexFlatL2.\n",
    "- Make sure the input text is in the form of a list of strings so that the encode runs smoothly.\n",
    "\n",
    "The main purpose of this function: Build a meaning-based fast search structure for use in RAG (Retrieval-Augmented Generation) or data-driven question and answer systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57392954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make function to build FAISS index with cosine similarity\n",
    "def build_faiss_index_cosine(teks):\n",
    "    # Section for performing embeddings\n",
    "    embedding = model.encode(teks , convert_to_numpy=True)\n",
    "\n",
    "    # Perform cosine calculation\n",
    "    embedding = embedding / np.linalg.norm(embedding, axis=1, keepdims=True)\n",
    "    embedding = embedding.astype('float32')\n",
    "\n",
    "    # Indexing\n",
    "    dimension = embedding.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embedding)\n",
    "\n",
    "    return index, embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9255cdc",
   "metadata": {},
   "source": [
    "Function: Retrieve Relevant Data from FAISS Index\n",
    "\n",
    "Function Explanation:\n",
    "The retrieve() function is used to retrieve the most relevant rows of data against a query based on meaning similarity using the FAISS index.\n",
    "\n",
    "Detailed Steps:\n",
    "1. Encode and Normalize Query:\n",
    "\n",
    "   `query_embedding = model.encode([query], convert_to_numpy=True)`\n",
    "   `query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)`\n",
    "   `query_embedding = query_embedding.astype(\"float32\")`\n",
    "\n",
    "   - The query is converted into a vector (embedding).\n",
    "   - The query vector is normalized so that it can be compared with the embedding data based on cosine similarity.\n",
    "   - Conversion to float32 as FAISS only supports this format.\n",
    "2. Search the FAISS Index:\n",
    "   \n",
    "   `scores, indices = index.search(query_embedding, top_k)`\n",
    "\n",
    "   - Searches the index to find the top_k most similar results.\n",
    "   - scores: Similarity or distance values (higher if cosine similarity).\n",
    "   - indices: Index of rows from the original data that are most relevant.\n",
    "3. Fetch Data from DataFrame:\n",
    "   \n",
    "   `result_df = df.iloc[indices[0]].copy()`\n",
    "   `result_df['similarity_score'] = scores[0]`\n",
    "\n",
    "   - Retrieve rows from the original DataFrame (df) based on the FAISS result.\n",
    "   - index.- Adds a new column similarity_score to display the similarity score.\n",
    "4. Return Results:\n",
    "   \n",
    "   `return result_df`\n",
    "\n",
    "   - Returns a DataFrame containing relevant data rows and similarity scores.\n",
    "\n",
    "Final Destination:\n",
    "This function is the core part of the RAG (Retrieval-Augmented Generation) system, which enables the model to answer the most relevant data-driven questions quickly and accurately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fdc384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make function to retrieve data from FAISS index\n",
    "def retrieve(query, index, df, top_k=3):\n",
    "    # 1. Encode dan normalization query\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)\n",
    "    query_embedding = query_embedding.astype(\"float32\")\n",
    "\n",
    "    # 2. Search to FAISS\n",
    "    scores, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    # 3. Retrieve the data row according to the indexing result\n",
    "    result_df = df.iloc[indices[0]].copy()\n",
    "    result_df['similarity_score'] = scores[0]\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52048ea",
   "metadata": {},
   "source": [
    "Function: Generate Answers Using OpenAI GPT\n",
    "\n",
    "Function Explanation:\n",
    "The generate_answer() function is used to generate text-based answers using the OpenAI ChatCompletion API (GPT-4.1-mini model). It combines the user's question and relevant context data to generate an informative and specific answer.\n",
    "\n",
    " Line-by-Line Explanation:\n",
    "1. Set the OpenAI API Key:\n",
    "   `openai.api_key = api_key`\n",
    "\n",
    "   The API key is set so that the request can be authenticated and the OpenAI service can be used.\n",
    "2. Set up a Prompt System:\n",
    "\n",
    "   `system_message = \"Kamu adalah asisten cerdas yang menjawab pertanyaan berdasarkan data yang diberikan.\"`\n",
    "   \n",
    "   System messages to direct the model to act as a data-driven intelligent assistant.\n",
    "3. Compose Prompts from Users:\n",
    "   \n",
    "   `user_message = f\"\"\"`\n",
    "   `Pertanyaan: {query}`\n",
    "\n",
    "   `Data yang relevan:`\n",
    "\n",
    "   `{context}`\n",
    "   \n",
    "   `\"\"\"`\n",
    "\n",
    "   Combining questions and relevant data (results from FAISS retrieval) into a prompt format for the model.\n",
    "\n",
    "4. Send Request to OpenAI:\n",
    "   \n",
    "   `response = openai.ChatCompletion.create(`\n",
    "\n",
    "    `model=\"gpt-4.1-mini\",`\n",
    "    \n",
    "    `messages=[...],`\n",
    "    \n",
    "    `temperature=0.3,`\n",
    "    \n",
    "    `max_tokens=1000`\n",
    "    \n",
    "    `)`\n",
    "\n",
    "    - Model: The model used (in this case GPT-4.1-mini).\n",
    "    - messages: A list of messages to set the context of the conversation. \n",
    "    - temperature=0.3: Creativity control. The lower it is, the more deterministic the output.\n",
    "    - max_tokens=1000: Limit on the length of the generated answer.\n",
    "\n",
    "5. Returning Answers:\n",
    "   \n",
    "   `return response.choices[0].message[\"content\"]`\n",
    "\n",
    "   Retrieve the answer from the result provided by OpenAI and return it as text.\n",
    "\n",
    "Function Objective:\n",
    "Connecting the data retrieval process with GPT's reasoning capabilities, so that the system can answer questions based on semantically discovered information from the dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07ee6aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def generate_answer(query, context, api_key):\n",
    "    # to enter the api key of the generative model being used\n",
    "    openai.api_key = api_key\n",
    "    # to tell you specifically what the generative model needs to do.\n",
    "    system_message = \"Kamu adalah asisten cerdas yang menjawab pertanyaan berdasarkan data yang diberikan.\"\n",
    "    # for users to input questions or data that they want to learn.\n",
    "    user_message = f\"\"\"\n",
    "    Pertanyaan: {query}\n",
    "\n",
    "    Data yang relevan:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4.1-mini\", # the model being used\n",
    "        # system messages or to process the input data or user\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        # to adjust the level of randomness in the next word prediction\n",
    "        temperature=0.3,\n",
    "        # to set the maximum number of tokens that can be processed\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f42a16",
   "metadata": {},
   "source": [
    "Generate Answers Using GPT Based on Relevant Data\n",
    "\n",
    "Explanation:\n",
    "The generate_answer() function is called to answer a user-specific question based on the data set provided in the text fields of the ecommerce DataFrame.\n",
    "Parameters:\n",
    " - `query`:\n",
    "   The user question that the model wants to answer.\n",
    "   In this example:\n",
    "   ‚Äúwhat segment makes the most purchases in the state of California?‚Äù\n",
    "\n",
    "- `context`:\n",
    "   A set of relevant data in text form (usually the result of multiple columns combined), used as context for the GPT model to answer based on real information.\n",
    "   Here: ecommerce[‚Äòtext‚Äô].\n",
    "\n",
    "- `api_key`:\n",
    "  The API key used to access the OpenAI ChatCompletion API. Must be provided for the request to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce57dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer(query= \"What is the total sales for State = CA, Ship Mode = First Class, and Segment = Home Office?\",\n",
    "                context=  ecommerce['teks'],  \n",
    "                api_key= \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6e03a4",
   "metadata": {},
   "source": [
    "## Auto Detect Column\n",
    "\n",
    "The `auto_detect_columns_info(df)` function aims to generate a summary of the data structure (schema) of a DataFrame df in text form, so that it can be used as a context for LLM models (such as GPT) to understand the data content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "541d43c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_detect_columns_info(df): \n",
    "    context_info = \"Schema kolom dari data:\\n\" # Initialize the context_info string that will contain all the column information.\n",
    "    for col in df.columns: # Loop to each column in the DataFrame\n",
    "        try:\n",
    "            sample_val = df[col].dropna().iloc[0] #  Take one instance of a value from a column that is not NaN (dropna()), to test if it is a date.\n",
    "            is_date = False\n",
    "            # Coba parse datetime\n",
    "            try:\n",
    "                pd.to_datetime(sample_val) # Try converting the instance value to datetime. If successful, it is considered that the column is of type time (datetime).\n",
    "                is_date = True\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            dtype = \"datetime\" if is_date else str(df[col].dtype)\n",
    "        except:\n",
    "            dtype = str(df[col].dtype) # If error when retrieving sample, fallback to default data type (df[col].dtype).\n",
    "\n",
    "        # Ambil 3 sample unik (stringify agar tidak error dengan tipe aneh)\n",
    "        sample = list(map(str, df[col].dropna().unique()[:3])) # Take the first 3 unique values from the column (without NaN), and convert them to strings for safe display (e.g. number, date, object).\n",
    "        context_info += f\"- {col} (type: {dtype}, sample: {sample})\\n\" # Add the detection result for this column to the context_info string.\n",
    "    return context_info # Returns a string containing all the column information, which can be used as a context prompt for LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337dba0e",
   "metadata": {},
   "source": [
    "## Generate Visualization\n",
    "\n",
    "The execute_plot(args, df) function is an important component of Streamlit-based data visualization applications. Its purpose is to generate graphs according to the dynamic instructions of the LLM model or the user, based on the data df and the plotting parameters args."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952e547d",
   "metadata": {},
   "source": [
    "What Does This Function Do?\n",
    "This function:\n",
    " - Reads visualization parameters (args) such as plot type (plot_type), x/y column, aggregation (agg), and filter.\n",
    " - Prepares data (filter, change datetime type, aggregation).\n",
    " - Create a graph according to the plot type: bar, line, scatter, pie, timeseries, histogram, or correlation_matrix.\n",
    " - Display chart results in the UI with Streamlit's st.pyplot()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f5a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_plot(args, df):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    plot_type = args.get(\"plot_type\") # Retrieve Parameters from args\n",
    "    x = args.get(\"x\")\n",
    "    y = args.get(\"y\")\n",
    "    agg = args.get(\"agg\", \"sum\")\n",
    "    filters = args.get(\"filters\", {})\n",
    "\n",
    "    if not plot_type or plot_type not in [\"bar\", \"line\", \"scatter\", \"pie\", \"timeseries\", \"histogram\", \"correlation_matrix\"]: # Check if Plot Type is Valid\n",
    "        st.warning(f\"üìõ Type plot '{plot_type}' not recognized or not yet supported.\")\n",
    "        return\n",
    "\n",
    "    df_filtered = df.copy()\n",
    "\n",
    "    # --- Convert date if there is a date field\n",
    "    for col in df_filtered.columns:\n",
    "        if \"date\" in col.lower() or \"tanggal\" in col.lower():\n",
    "            try:\n",
    "                df_filtered[col] = pd.to_datetime(df_filtered[col], errors='coerce')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # --- Apply filters\n",
    "    for col, val in filters.items():\n",
    "        if col not in df_filtered.columns:\n",
    "            continue\n",
    "        if isinstance(val, list):\n",
    "            df_filtered = df_filtered[df_filtered[col].isin(val)]\n",
    "        elif isinstance(val, str) and val.isdigit() and 'date' in col.lower():\n",
    "            df_filtered = df_filtered[df_filtered[col].dt.year == int(val)]\n",
    "        else:\n",
    "            df_filtered = df_filtered[df_filtered[col] == val]\n",
    "\n",
    "    # --- Handle Empty Data\n",
    "    if df_filtered.empty:\n",
    "        st.warning(\"üì≠ No data matching the filter.\")\n",
    "        return\n",
    "\n",
    "    # --- Aggregation (if required)\n",
    "    if plot_type in [\"bar\", \"line\", \"pie\", \"timeseries\"] and x and y:\n",
    "        if agg == \"sum\":\n",
    "            grouped = df_filtered.groupby(x)[y].sum().reset_index()\n",
    "        elif agg == \"mean\":\n",
    "            grouped = df_filtered.groupby(x)[y].mean().reset_index()\n",
    "        elif agg == \"median\":\n",
    "            grouped = df_filtered.groupby(x)[y].median().reset_index()\n",
    "        elif agg == \"count\":\n",
    "            grouped = df_filtered.groupby(x)[y].count().reset_index()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported aggregation: {agg}\")\n",
    "    else:\n",
    "        grouped = df_filtered\n",
    "\n",
    "    # --- Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    if plot_type == \"bar\":\n",
    "        plt.bar(grouped[x], grouped[y])\n",
    "        plt.title(f\"Bar Chart of {y} by {x} ({agg})\")\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "    elif plot_type == \"line\":\n",
    "        plt.plot(grouped[x], grouped[y], marker='o')\n",
    "        plt.title(f\"Line Chart of {y} by {x} ({agg})\")\n",
    "\n",
    "    elif plot_type == \"scatter\":\n",
    "        plt.scatter(grouped[x], grouped[y])\n",
    "        plt.title(f\"Scatter Plot: {y} vs {x}\")\n",
    "\n",
    "    elif plot_type == \"pie\":\n",
    "        plt.pie(grouped[y], labels=grouped[x], autopct='%1.1f%%')\n",
    "        plt.title(f\"Pie Chart of {y} by {x} ({agg})\")\n",
    "\n",
    "    elif plot_type == \"timeseries\":\n",
    "        grouped[x] = pd.to_datetime(grouped[x], errors='coerce')\n",
    "        grouped = grouped.sort_values(x)\n",
    "        plt.plot(grouped[x], grouped[y])\n",
    "        plt.title(f\"Time Series: {y} over {x} ({agg})\")\n",
    "\n",
    "    elif plot_type == \"histogram\":\n",
    "       numeric_cols = df_filtered.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "       if y:\n",
    "           if isinstance(y, list):  # can be from LLM as a list\n",
    "               cols_to_plot = [col for col in y if col in numeric_cols]\n",
    "           else:\n",
    "               cols_to_plot = [y] if y in numeric_cols else []\n",
    "       else:\n",
    "           cols_to_plot = numeric_cols\n",
    "\n",
    "       if not cols_to_plot:\n",
    "           st.error(\"üìõ No numeric column can be used for histogram.\")\n",
    "           return\n",
    "\n",
    "       for col in cols_to_plot:\n",
    "           sns.histplot(df_filtered[col], kde=True)\n",
    "           plt.title(f\"Histogram of {col}\")\n",
    "           plt.xlabel(col)\n",
    "           plt.tight_layout()\n",
    "           st.pyplot(plt)\n",
    "           plt.clf()\n",
    "       return\n",
    "\n",
    "\n",
    "    elif plot_type == \"correlation_matrix\":\n",
    "        numeric_df = df_filtered.select_dtypes(include='number')\n",
    "        corr = numeric_df.corr()\n",
    "        sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "        plt.title(\"Correlation Matrix\")\n",
    "\n",
    "    else:\n",
    "        st.error(f\"Type plot '{plot_type}' not recognized or not yet supported.\")\n",
    "        return\n",
    "\n",
    "    plt.tight_layout()\n",
    "    st.pyplot(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc820c86",
   "metadata": {},
   "source": [
    "Benefits of This Function:\n",
    " - Flexible to various data types and queries.\n",
    " - Supports automatic plotting of LLM instructions.\n",
    " - Can be combined with analysis such as generate_auto_answer() to create narrative answers from graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3923c00f",
   "metadata": {},
   "source": [
    "## Interpret Chart\n",
    "\n",
    "The function `generate_auto_answer(df, args)` is a data-driven automatic analysis that:\n",
    "- Reads the visualization type (plot_type) and parameters from args (e.g. from LLM).\n",
    "- Performs basic numerical/statistical analysis depending on the graph type.\n",
    "- Generates narrative (natural language) answers that match the content of the graph that will be/was just displayed.\n",
    "- Very useful for your system to explain graphs without LLM, based only on the data and visualization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbb9a541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_auto_answer(df, args):\n",
    "    try:\n",
    "        plot_type = args.get(\"plot_type\")\n",
    "        x = args.get(\"x\")\n",
    "        y = args.get(\"y\")\n",
    "        agg = args.get(\"agg\", \"sum\")\n",
    "        filters = args.get(\"filters\", {})\n",
    "\n",
    "        # Filter data\n",
    "        for k, v in filters.items():\n",
    "            if k in df.columns:\n",
    "                df = df[df[k] == v]\n",
    "\n",
    "        if df.empty:\n",
    "            return \"Empty data after filtering.\"\n",
    "\n",
    "        # Bar, Line, Timeseries\n",
    "        if plot_type in [\"bar\", \"line\", \"timeseries\"]:\n",
    "            if x not in df.columns or y not in df.columns:\n",
    "                return f\"columns {x} or {y} Not found in the data.\"\n",
    "\n",
    "            grouped = df.groupby(x)[y].agg(agg).reset_index()\n",
    "            grouped = grouped.sort_values(by=y, ascending=False)\n",
    "            top = grouped.iloc[0]\n",
    "            percent = round(100 * top[y] / grouped[y].sum(), 2)\n",
    "\n",
    "            return f\"Category '{top[x]}' have {agg} {y.lower()} highest, i.e {top[y]:,.2f} ({percent}%) compared to other categories.\"\n",
    "\n",
    "        # Scatter plot\n",
    "        elif plot_type == \"scatter\":\n",
    "            if x not in df.columns or y not in df.columns:\n",
    "                return f\"columns {x} or {y} Not found in the data.\"\n",
    "\n",
    "            corr = df[[x, y]].corr().iloc[0, 1]\n",
    "            return f\"There is a correlation of {corr:.2f} between '{x}' and '{y}'.\"\n",
    "\n",
    "        # Pie chart\n",
    "        elif plot_type == \"pie\":\n",
    "            if x not in df.columns or y not in df.columns:\n",
    "                return f\"columns {x} or {y} Not found in the data.\"\n",
    "\n",
    "            pie_group = df.groupby(x)[y].agg(agg).reset_index()\n",
    "            top = pie_group.sort_values(by=y, ascending=False).iloc[0]\n",
    "            percent = round(100 * top[y] / pie_group[y].sum(), 2)\n",
    "            return f\"Category '{top[x]}' has contribution {agg} {y.lower()} highest i.e {top[y]:,.2f} ({percent}%).\"\n",
    "\n",
    "        # Correlation Matrix\n",
    "        elif plot_type == \"correlation_matrix\":\n",
    "            numeric_cols = df.select_dtypes(include='number').columns\n",
    "            corr_matrix = df[numeric_cols].corr()\n",
    "            high_corr = corr_matrix.where(~np.eye(len(corr_matrix),dtype=bool)).abs().stack().sort_values(ascending=False)\n",
    "            if not high_corr.empty:\n",
    "                top_pair = high_corr.idxmax()\n",
    "                value = high_corr.max()\n",
    "                return f\"The highest correlation is between '{top_pair[0]}' and '{top_pair[1]}', with value {value:.2f}.\"\n",
    "            else:\n",
    "                return \"No significant correlation found between numeric columns.\"\n",
    "\n",
    "        # Histogram\n",
    "        elif plot_type == \"histogram\":\n",
    "            numeric_cols = df.select_dtypes(include='number').columns.tolist()\n",
    "            return f\"The histogram will show the distribution of the following numerical columns: {', '.join(numeric_cols)}.\"\n",
    "\n",
    "        else:\n",
    "            return f\"Type plot '{plot_type}' not recognized.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Fails to generate auto-answer: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6948342d",
   "metadata": {},
   "source": [
    "When is this function used?\n",
    " - When you want to explain the graph automatically (without LLM).\n",
    " - To synchronize plots and answers, especially when datasets are uploaded freely (they can be different).\n",
    " - As a fallback if there is no good answer from LLM, or as a complement to the answer from LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720f1f2c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This project demonstrates a fully automated Data Analytics Assistant that integrates:\n",
    "- Retrieval-Augmented Generation (RAG) for understanding both structured (CSV) and unstructured (PDF) data.\n",
    "- Multi-chart generation with support for various plot types (bar, line, pie, scatter, time series, histogram, and correlation matrix).\n",
    "- Dynamic function calls generated via LLM (GPT-4o) based on user questions and column schema.\n",
    "- A robust fallback mechanism using a custom generate_auto_answer() function to provide reliable and explainable data insights, even without depending entirely on LLM-generated text.\n",
    "\n",
    "Key strengths of the system include:\n",
    "- Flexibility to handle any uploaded dataset regardless of column names or types.\n",
    "- Smart syncing of visualizations with interpretable natural language answers\n",
    "- Compatibility with both quantitative data (CSV) and qualitative text (PDF) in a unified Streamlit interface.\n",
    "\n",
    "This solution reflects not only strong integration of LLM + Data Science + UI, but also real-world considerations such as:\n",
    "- Handling missing columns and schema mismatches.\n",
    "- Producing meaningful insights for business questions.\n",
    "- Ensuring plot and answer coherence across multiple chart types.\n",
    "\n",
    "‚úÖ In summary, this portfolio showcases how LLMs can be enhanced with structured logic to build intelligent, general-purpose data assistants‚Äîready for business analytics, automated reporting, or even client-facing dashboards."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
